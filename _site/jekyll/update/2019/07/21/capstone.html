<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Create a Customer Segmentation Report for Arvato Financial Services | Bruno VC Martins</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Create a Customer Segmentation Report for Arvato Financial Services" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Capstone Project" />
<meta property="og:description" content="Capstone Project" />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2019/07/21/capstone.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2019/07/21/capstone.html" />
<meta property="og:site_name" content="Bruno VC Martins" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-21T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Capstone Project","@type":"BlogPosting","url":"http://localhost:4000/jekyll/update/2019/07/21/capstone.html","headline":"Create a Customer Segmentation Report for Arvato Financial Services","dateModified":"2019-07-21T00:00:00-06:00","datePublished":"2019-07-21T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2019/07/21/capstone.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
<!-- <link href='https://fonts.googleapis.com/css?family=Arvo:400,400italic,700,700italic' rel='stylesheet' type='text/css'> -->
<!-- <link href="https://fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet">  -->
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Bruno VC Martins" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Bruno VC Martins</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Create a Customer Segmentation Report for Arvato Financial Services</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-07-21T00:00:00-06:00" itemprop="datePublished">Jul 21, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="capstone-project">Capstone Project</h2>

<h3 id="introduction"><strong>Introduction</strong></h3>

<p>This post discusses in detail a <a href="https://github.com/bvcmartins/dsnd_capstone">solution</a> for the project “Create a Customer Segmentation Report for Arvato Financial Services”, which is one of options for the Data Scientist Nanodegree capstone projects. It is in fact a continuation of a previous project whose solution was posted <a href="https://github.com/bvcmartins/dsndProject3">here</a>. I chose it because of its broad scope which involves a reasonably complex data cleaning procedure, unsupervised learning, analysis of imbalanced data, and prediction using supervised learning tools. Below I will discuss the solution and the difficulties faced in some of the most challenging steps.</p>

<p>The objective of this project was to predict, based on a broad list of features, if someone who is receiving a mailout campaign would become a customer of the company. It was divided into three parts:</p>

<ol>
  <li>Segment the data obtained for the general population of Germany in clusters and identify in which of these clusters the current customers fall into</li>
  <li>Select people from these clusters who are not customers to receive a mailout campaign and train a supervised learning model to predict who will become a new customer</li>
  <li>Apply the model to a Kaggle competition</li>
</ol>

<p>In part 1, I carried out the data segmentation using PCA (Principal component analysis) for dimensionality reduction and K-Means for clustering. In part 2, I used three tree-based supervised learning techniques (Random Forests, AdaBoost and XGBoost) models for the prediction of new customers.</p>

<p>The best model was tested against the test set through a Kaggle competition and the scores were ranked using the AUC (area under curve) metric. This metric was used because of the highly imbalanced distribution of the response variable. However, in the last part of this report, I will show why this metric is not the best choice for this problem and why the F1 metric would be more suited to measure model performance.</p>

<h3 id="the-dataset"><strong>The dataset</strong></h3>

<p>Arvato kindly provided us four datasets:</p>

<ul>
  <li>
    <p>Azdias: sample of the general German population categorized according to a variety of features involving personality traits, demographics and financial information (891,221 entries, 366 features).</p>
  </li>
  <li>
    <p>Customers: classification of current customers according to the same features used for Azdias (191,652 entries, 369 features). It also contains customer categorization and information about purchase preferences.</p>
  </li>
  <li>
    <p>Mailout_train: training set containing potential customers chosen to receive a mailout campaign (42,962 entries, 367 features). It also contains information if the person became a customer (target variable).</p>
  </li>
  <li>
    <p>Mailout_test: testing set for the supervised learning model (42,833 entries, 366 features).</p>
  </li>
</ul>

<p>Two support files for the interpretation of the features were also provided:</p>

<ul>
  <li>
    <p>DIAS Attributes - Values 2017: information about code levels for some of the features.</p>
  </li>
  <li>
    <p>DIAS Information Levels - Attributes 2017: high-level information about most (but not all) features.</p>
  </li>
</ul>

<p>Most of the features are ordinal and the numbers represent a label for ranked value levels. As far as I could identify, all columns marked as float are actually of type int64 mixed with NaN symbols. In fact, NaNs are treated as float64 and, because of its higher precedence, they make the entire column being cast as float. Most recent Pandas versions (higher than 0.24.0) incorporated the new type Int64 which supports integer NaN. Despite being somewhat experimental, I used this new data type as a way to reduce the large amount of ancillary code required to control operations over different data types.</p>

<p>There are also 6 features of type object. These are categorical variables, except for EINGEFUEGT_AM, which is datetime. I dropped this feature because it did not have any explicative power.</p>

<p>Most of the columns contained NaNs. Actually, NaNs comprised almost 10% of all data.</p>

<h3 id="data-cleaning-and-processing"><strong>Data cleaning and processing</strong></h3>

<p>Cleaning this dataset was a relatively complex task mainly because of the number and diversity of features. The steps are outlined below:</p>

<ul>
  <li>pre-cleaning</li>
  <li>converting missing values to NaN</li>
  <li>assessing missing data per feature</li>
  <li>assessing missing data per row</li>
  <li>imputing missing data</li>
  <li>re-encoding mixed features</li>
  <li>one-hot encoding categorical features</li>
  <li>scaling numerical features</li>
  <li>removing outliers</li>
</ul>

<h4 id="pre-cleaning"><strong>Pre-cleaning</strong></h4>

<p>In this first cleaning round, I applied general-purpose operations like converting all numeric features to Int64 (support to integer NaN) and made substitutions of some non-standard missing data codes.</p>

<h4 id="converting-missing-values-to-nan"><strong>Converting missing values to NaN</strong></h4>

<p>The challenge with this step was that the missing data codes were feature-dependent. Most of the missing values were coded as -1 or 0, but some of them were coded (and not listed in file DIAS Attributes) as strings X or XX. I converted these special codes into NaNs during pre-cleaning.</p>

<p>For missing values coded as -1 or 0, I first converted them into a different code (-100) to data type-related problems and then cast them to integer NaN.</p>

<h4 id="assessing-missing-data-per-feature"><strong>Assessing missing data per feature</strong></h4>

<p>After having all missing values converted into NaN I was able to assess which features had more than 445,000 missing. This number is half the total number of entries. As shown below, I found 9 features satisfying this requirement. They corresponded to 18% of all missing values and were all dropped.</p>

<center>
<img src="/images/capstone/missing_feature.png" width="400" />
</center>

<h4 id="assessing-missing-data-per-row"><strong>Assessing missing data per row</strong></h4>

<p>In the same way certain columns can have a disproportional amount of missing values, this can also happen to groups of rows. As shown in histogram below, the distribution of missing data per row is multimodal. Most of the rows have less than 50 NaNs, but some of them can have at least 180 missing features. I singled out these outliers for further assessment.</p>

<center>
<img src="/images/capstone/missing_row.png" width="800" />
</center>

<p>To decide if these rows should be dropped or imputed, I applied the <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov-Smirnov test</a>. This non-parametric test checks if two groups come from the same distribution. To apply it, I selected all features that were numeric and that had no missing values in both groups. Then I compared both groups for all the features. The null hypothesis for each comparison was that both groups were identical.</p>

<p>Because the test involved multiple comparisons, I also applied the very strict <a href="https://en.wikipedia.org/wiki/Bonferroni_correction">Bonferroni correction</a> to the p-values.</p>

<p>Results for each feature are shown below. The difference between the two groups was significant only for 8.2% of the test features. Note that this number is not a p-value and, as such, it should not be compared with the 0.05 significance level. The test was just a guide and not a proper statistical-based decision making process. In the end, I decided that 8.2% was not reason enough to drop the rows.</p>

<center>
<img src="/images/capstone/ks_test.png" width="800" />
</center>

<h4 id="imputing-missing-data"><strong>Imputing missing data</strong></h4>

<p>I carried out imputation separately for numeric and object variables. Numeric variables were imputed using the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html">Simple Imputer</a> with strategy median value. Object-type variables had their missing values replaced by the column mode using pandas <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html">get_dummies</a> method.</p>

<p>I also performed a few exploratory tests with the new <a href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html">Iterative Imputer</a> which trains a model to predict the ideal value. K-nearest neighbor was used as the imputing model but the results were similar to conventional Simple Imputer at a much higher computational cost.</p>

<h4 id="re-encoding-mixed-features"><strong>Re-encoding mixed features</strong></h4>

<p>After removing all NaNs and outliers, next step was to re-encode variables of mixed type. These were:</p>

<ul>
  <li>PRAEGENDE_JUGENDJAHRE</li>
  <li>CAMEO_INTL_2015</li>
  <li>LP_LEBENSPHASE_FEIN</li>
  <li>LP_LEBENSPHASE_GROB</li>
</ul>

<p>I could also have re-encoded variable PLZ8_BAUMAX but the explanatory gain was too small. Detailed descriptions of the derived variables and their levels are provided in the <a href="">notebook</a>.</p>

<h4 id="one-hot-encoding"><strong>One-hot encoding</strong></h4>

<p>So far I cleaned all data and re-encoded mixed variables. Next, I applied <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html">one-hot encoding</a> to all 13 categorical variables. Binary variables were left out.</p>

<h4 id="scaling"><strong>Scaling</strong></h4>

<p>Next I applied scaling all numeric variables (including ordinals). At first I used Standard Scaler, but it was too sensitive to outliers. Later I figured out that a combination of the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html">Robust Scaler</a> with outlier removal greatly improved the prediction results.</p>

<h4 id="removing-outliers"><strong>Removing outliers</strong></h4>

<p>This step was fundamental to improve the predictor performance in supervised learning. I removed the outliers following a three-step process:</p>

<ul>
  <li>z-transform all scaled variables</li>
  <li>mark all elements with an absolute distance greater than 4.0.</li>
  <li>remove all rows with at least one outlier</li>
</ul>

<p>The threshold distance of 4.0 is actually larger than the standard inter-quartile range (IQR) distance of 3.0. I applied it because IQR was too strict and eliminated too many rows.</p>

<p>I also did some exploratory tests with two native scikit-learn outlier removal methods: Isolation Forest and Local Outlier Factor. They failed to eliminate most of the outliers. Both have tuning parameters because they use machine learning methods to identify the outliers. It is quite possible that I could have gotten a better result if I had properly tuned them.</p>

<h3 id="customer-segmentation-report"><strong>Customer segmentation report</strong></h3>

<p>The objective of the first part of the project was to reduce the dimensionality of the Azdias dataset (sample of general population of Germany) and to categorize the population in clusters. Then I applied the same transformations to the Customers dataset and identified the clusters with excess customer population. By following this procedure, I identified which clusters were the most important for new customers identification in the general population.</p>

<p>The analysis comprised the following steps:</p>

<ol>
  <li>Reduce dimensionality of the general population dataset using PCA</li>
  <li>Clusterize the reduced space using K-means in order to identify customer segments</li>
  <li>Apply the PCA transformation defined in step 1 to the Customers dataset</li>
  <li>Clusterize the customer reduced space and identify which clusters have a population excess</li>
</ol>

<h4 id="pca"><strong>PCA</strong></h4>

<p>After being cleaned, one-hot encoded and scaled, the number of features increased to 496. Some of these features were redundant and many of them were not important at all for predictions. It is interesting, at this point, to reduce dimensionality so we could work on a leaner dataset that was more suitable to visualization and to training.</p>

<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">PCA</a> is the most basic tool for dimensionality reduction. I applied it to reduce the number of dimensions from the original 496 to 107. This is the minimum number of dimensions needed to explain 80% of the variance. It still quite large but, as we can see in the scree plot below, only about 10 dimensions were actually individually relevant for the explained variance.</p>

<center>
<img src="/images/capstone/screeplot_pca.png" width="800" />
</center>

<h4 id="k-means"><strong>K-Means</strong></h4>

<p>Next, after reducing the number of dimensions of the data space, I applied k-means to generate clusters containing similar instances. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">K-means</a> is by far the most straightforward clustering approach to this problem and it basically requires us only to define the number of clusters. It is also a greedy method so all points must be classified.</p>

<p>Choosing the number of clusters requires a trade-off between model size and distance to cluster center. In general the best value occurs at the inflexion point of the K-means score which, as shown in the scree plot below, was around 12 clusters for this dataset.</p>

<p>One interesting point to make here is that we could have performed a more formal analysis of the ideal number of clusters using the silhouette score. However, the calculation involves a too high computational cost which prevented its application.</p>

<p><img src="/images/capstone/screeplot_kmeans.png" alt="" /></p>

<p>The barplot below shows how all elements of Azdias were distributed among the 12 clusters. We can see that the distribution was approximately balanced, which is a good indication that the method is consistent.</p>

<center>
<img src="/images/capstone/general_pop.png" width="400" />
</center>

<p>One of the main advantages of using PCA is that it allows us to inspect projections of a high-dimensional dataset. Because the two first components are so strong we could plot all the data points on this 2D space in order to get a better idea of how the data distributed and how the clusters were assigned. That is exactly what is shown in the plot below.</p>

<p><img src="/images/capstone/general_clusters.png" alt="" /></p>

<p>Note that the data distribution has a homogeneous oval shape which is not quite separable. There are no evident spatially-separated clusters. If we look at the color map identifying different clusters, it is clear that K-Means performed a somewhat arbitrary cluster separation. Actually this method is probably not the most adequate for this dataset. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html">DBSCAN</a> would be a more adequeate choice; however, the high computational cost prevented its application since it requires many iterations to adjust its two main parameters.</p>

<p>In past iterations of this project, I also tried to other dimensionality reduction techniques applied after PCA: T-SNE and UMAP. Some of the results were promising but not-consistent enough to be considered for this project.</p>

<h3 id="application-to-customers-dataset"><strong>Application to Customers dataset</strong></h3>

<p>The data cleaning and processing steps described above actually defined a pipeline, which can be applied to any dataset having the same features as Azdias.</p>

<p>Customers dataset had the same structure as Azdias with 3 extra columns (PRODUCT_GROUP, CUSTOMER_GROUP, ONLINE_PURCHASE). We dropped these columns because they did not have any relevant information for the model training.</p>

<p>By applying the pipeline to the rest of the Customers dataset, we obtained the cluster population distribution shown in the graph below.</p>

<center>
<img src="/images/capstone/customer_clusters.png" width="400" />
</center>

<p>That is a very interesting result. The Customers dataset, when projected onto the clusters defined using Azdias, showed a population excess in clusters 0 and 10. This result suggested that current customers of the company share some common traits and can be singled out from the general population. If we revert this thought and look back at Azdias, all of its elements falling in clusters 0 or 10 are the ones that should be targeted by the mailout campaign.</p>

<p>Another interesting consequence of this result is that we inferred some of the actual customers common traits. Looking at the PCA components weights for clusters 0 and 10 as plotted below, we note that only a few of them were relevant for the description of the cluster.</p>

<center>
<img src="/images/capstone/kmeans_comps.png" width="800" />
</center>

<p>Take cluster 0 as an example. Clearly components 0, 2 and 3 were the most important. Each of these components is an axis with a positive and a negative orientation. Both must be considered. The positive and negative features grow together and inversely to each other.</p>

<p>Let’s look at the strongest attributes for components 0, 2 and 3.</p>

<p><strong>Component 0: top 5 positive attributes</strong></p>

<ol>
  <li>PLZ8_ANTG3: number of 6-10 family house in the PLZ8</li>
  <li>KBA13_ANTG3: undefined. Probably number of 6-10 family houses in the KBA13</li>
  <li>KBA13_ANTG4: undefined. Probably number of &gt;10 family house in the KBA13</li>
  <li>PLZ8_ANTG4: number of &gt;10 family house in the PLZ8</li>
  <li>KBA13_BAUMAX: undefined. Probably related to housing</li>
</ol>

<p><strong>Component 0: top 5 negative attributes</strong></p>

<ol>
  <li>KBA13_AUTOQUOTE: share of cars per household within the PLZ8</li>
  <li>KBA05_ANTG1: number of 1-2 family houses in the cell</li>
  <li>MOBI_REGIO: movement patterns</li>
  <li>KBA13_ANTG1: number of 1-2 family houses in the KBA13</li>
  <li>PLZ8_ANTG1: number of 1-2 family houses in the PLZ8</li>
</ol>

<p>For component 0 the pattern is clear. Positive attributes were related to high-density housing while negative attributes were related to low-density housing, possibly in the suburbs because of the movement pattern feature.</p>

<p><strong>Component 2: top 5 positive attributes</strong></p>

<ol>
  <li>PRAEGENDE_JUGENDJAHRE_intervall: period when individual was born</li>
  <li>CJT_TYP_1: undefined</li>
  <li>FINANZ_SPARER: money saver</li>
  <li>CJT_TYP_2: undefined</li>
  <li>SEMIO_PFLICHT: affinity indicating in what way the person is dutyfull traditional minded</li>
</ol>

<p><strong>Component 2: top 5 negative attributes</strong></p>

<ol>
  <li>CJT_TYP_3: undefined</li>
  <li>CJT_TYP_4: undefined</li>
  <li>ALTERSKATEGORIE_GROB: age through prename analysis modelled on millions of first name-age-reference data</li>
  <li>CJT_TYP_5: undefined</li>
  <li>FINANZ_VORSORGER: financial typology: be prepared</li>
</ol>

<p>For component 2, both positive and negative attributes were probably related to conservative-minded older individuals. Too many attribute descriptions were missing.</p>

<p><strong>Component 3: top 5 positive attributes</strong></p>

<ol>
  <li>GEBURTSJAHR: birth year</li>
  <li>D19_VERSAND_ONLINE_QUOTE_12: amount of online transactions within all transactions in the segment mail-order</li>
  <li>D19_GESAMT_ONLINE_QUOTE_12: amount of online transactions within all transactions in the complete file</li>
  <li>D19_GESAMT_ANZ_24: undefined</li>
  <li>D19_VERSAND_ANZ_24 undefined</li>
</ol>

<p><strong>Component 3: top 5 negative attributes</strong></p>

<ol>
  <li>D19_GESAMT_ONLINE_DATUM: actuality of the last transaction with the complete file ONLINE</li>
  <li>D19_VERSAND_DATUM: actuality of the last transaction for the segment mail-order ONLINE</li>
  <li>D19_GESAMT_DATUM: actuality of the last transaction with the complete file TOTAL</li>
  <li>KOMBIALTER: undefined</li>
  <li>D19_KONSUMTYP_MAX: undefined</li>
</ol>

<p>For component 3 both positive and negative attributes were related to online transactions.</p>

<p>We could have gone deeper into this analysis using more than just the top 5 features and maybe parsing the results using Latent Dirichlet Allocation to identify topics associated with each of the clusters. However, this is outside the scope of this work.</p>

<h3 id="supervised-learning-model"><strong>Supervised Learning Model</strong></h3>

<p>The previous section was aimed at selecting new potential customers that would receive a mailout campaign. These were selected by Arvato which provided us with two extra datasets (mailout_train, mailout_test) containing just these individuals. Both had approximately the same size with around 43,000 entries. Mailout train is the training set and it contains the same features as Azdias (366 columns) and extra RESPONSE column indicating if the person became a customer of the company following the campaign. The other block was used to generate predictions.</p>

<p>As expected for this kind of study, the response classes were very imbalanced. The classes were:</p>

<ul>
  <li>0 - did not become customer after campaign</li>
  <li>1 - became customer after campaign</li>
</ul>

<p>Only 1.2% of the entries were of class 1.</p>

<p>If we compare the population distribution per cluster for response 0 and for response 1:</p>

<center>
<img src="/images/capstone/mailout_clusters_0.png" width="400" />
</center>

<center>
<img src="/images/capstone/mailout_clusters_1.png" width="400" />
</center>

<p>we observe that they are quite similar to each other and to the customers distribution. This means that:</p>

<ol>
  <li>the cluster number did not convey much information for differentiation between classes 0 and 1</li>
  <li>the elements that received the mailout campaign were indeed selected from the customers analysis. This was a good indication of the consistency of the method</li>
</ol>

<p>Another interesting visualization was to plot the data points projected on the first two PCA axes as shown below:</p>

<p><img src="/images/capstone/mailout_response.png" alt="" /></p>

<p>Note how the few points with reponse 1 (became clients) are embedded in the much larger homogeneous volume generated by the elements with response 0. Even if we plot only class 1 we find no relevant spatial seggregation of the minority class.</p>

<p><img src="/images/capstone/mailout_response_1.png" alt="" /></p>

<p>It is worthy pointing out that this is a place where T-SNE might be helpful. Some preliminary attempts were not well-succeeded but, in principle, this method could help separating the two classes by correlation.</p>

<h4 id="models"><strong>Models</strong></h4>

<p>Class imblance is a hard problem to deal with. Our first approach to this issue was to use <a href="https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html">SMOTE</a> to balance the training set. However, this is not without risks. Some important points to keep in mind are:</p>

<ul>
  <li>SMOTE must only be applied to training set. Predictions on Test set must use the original imbalanced classes</li>
  <li>if inside of a Cross-Validation loop, SMOTE must be calculated for every fold</li>
  <li>we applied SMOTE only to the PCA-transformed dataset because all features are numeric and real. For categorical variables SMOTENC must be used instead</li>
  <li>we performed oversampling of the minority class. We could also have performed undersampling of the majority class</li>
</ul>

<p>We first tested different models using their default parameters. For each model, we ran 5-fold CV training with and without SMOTE. The results obtained with the Test set (always without SMOTE) are shown below.</p>

<table>
  <thead>
    <tr>
      <th>Classifier</th>
      <th style="text-align: center">NO SMOTE F1</th>
      <th style="text-align: center">NO SMOTE AUC</th>
      <th style="text-align: center">SMOTE F1</th>
      <th style="text-align: center">SMOTE AUC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>AdaBoost</td>
      <td style="text-align: center">0.0</td>
      <td style="text-align: center">0.57</td>
      <td style="text-align: center">0.03</td>
      <td style="text-align: center">0.53</td>
    </tr>
    <tr>
      <td>Random Forests</td>
      <td style="text-align: center">0.0</td>
      <td style="text-align: center">0.50</td>
      <td style="text-align: center">0.03</td>
      <td style="text-align: center">0.52</td>
    </tr>
    <tr>
      <td>XGBoost</td>
      <td style="text-align: center">0.0</td>
      <td style="text-align: center">0.54</td>
      <td style="text-align: center">0.03</td>
      <td style="text-align: center">0.56</td>
    </tr>
  </tbody>
</table>

<p>That was not very compelling. The best optimized model, obtained after running the Grid Search routine, was the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html">AdaBoost Classifier</a> with learning_rate=1.0 and n_estimators=300.</p>

<p>Actually, running grid search with SMOTE is not easy. As we can see below, it requires the use of a SMOTE pipeline integrated with the training model. Scoring was AUC and we used 3-fold split instead of 5-fold to save processing time.</p>

<pre><code class="language-`python">
ab_model = Pipeline([
        ('sampling', SMOTE()),
        ('ab', AdaBoostClassifier())])

parameters = {"ab__learning_rate" : [0.9, 1.0, 1.1, 2.0],
              "ab__n_estimators": [100, 200, 300, 1000]}

kf = StratifiedKFold(n_splits=3)
grid_ab = GridSearchCV(ab_model, parameters, n_jobs=16, cv=kf, scoring='roc_auc', verbose=10)

grid_ab.fit(X, y)
</code></pre>

<p>Optimized results were slightly better but far from being good:</p>

<table>
  <thead>
    <tr>
      <th>Classifier</th>
      <th style="text-align: center">NO SMOTE F1</th>
      <th style="text-align: center">NO SMOTE AUC</th>
      <th style="text-align: center">SMOTE F1</th>
      <th style="text-align: right">SMOTE AUC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>AdaBoost Optimized</td>
      <td style="text-align: center">0.01</td>
      <td style="text-align: center">0.56</td>
      <td style="text-align: center">0.03</td>
      <td style="text-align: right">0.58</td>
    </tr>
  </tbody>
</table>

<p>SMOTE was clearly not the best solution for this problem. After many trial and error attempts using different models, two modifications greatly improved the results:</p>

<ol>
  <li>Use the scale_pos_weight XGBoost parameter for compensation of class imbalance</li>
  <li>Use the cleaned dataframe instead of the PCA-transformed one</li>
</ol>

<p>The second modification was surprising. In principle we should have expected that PCA contained the essence of the dataset but that was not the case. Probably the cuttoff of 80% explained variance was too low. The fact was that the only way we could get beyond AUC=0.7 was to use the full dataset. As we can see below, the results were much better now:</p>

<table>
<colgroup>
<col width="40%" />
<col width="30%" />
<col width="30%" />
</colgroup>
<thead>
<tr class="header">
<th>Classifier</th>
<th align="center">F1</th>
<th align="center">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td>XGBoost Optimized</td>
<td align="center">0.0</td>
<td align="center">0.77</td>
</tr>
</tbody>
</table>

<p>The best XGBoost model parameters after random search optimization were:</p>

<ul>
  <li>base_score=0.5</li>
  <li>booster=’dart’</li>
  <li>colsample_bylevel=1</li>
  <li>colsample_bynode=1</li>
  <li>colsample_bytree=0.7</li>
  <li>gamma=0</li>
  <li>learning_rate=0.01</li>
  <li>max_delta_step=0</li>
  <li>max_depth=6</li>
  <li>min_child_weight=1</li>
  <li>n_estimators=500</li>
  <li>nthread=None</li>
  <li>objective=’binary:logistic’</li>
  <li>random_state=34</li>
  <li>reg_alpha=0</li>
  <li>reg_lambda=1</li>
  <li>sample_pos_weight=80</li>
  <li>scale_pos_weight=1</li>
</ul>

<p>It is worthy noting that I also tried some variations of the dataset:</p>

<ul>
  <li>add clusters as one-hot encoded features</li>
  <li>add the customers dataset to class 1</li>
</ul>

<p>None of them increased the final score.</p>

<h3 id="prediction-and-kaggle-competition"><strong><em>Prediction and Kaggle competition</em></strong></h3>

<p>Finally, I used the best model to generate predictions with the Mailout_test and apply them to the Kaggle competition. The score was 0.79917, which is not bad but somewhat far from the first place. This result seemed good in principle but in fact it was very bad. I came to this conclusion after inspecting the confusion matrix obtained for this same model:</p>

<center>
<img src="/images/capstone/cm_model6.png" width="400" />
</center>

<p>This is telling us that all minority values were misclassified. It is basically just the Naive Classifier! And that was sufficient to get a score of almost 0.8! I consider that AUC was not a good metric choice for this problem and that F1 would have been more adequate. In fact, I ran grid search again optimizing the parameters to maximize F1 instead of AUC and I obtained the following results:</p>

<table>
<colgroup>
<col width="40%" />
<col width="30%" />
<col width="30%" />
</colgroup>
<thead>
<tr class="header">
<th>Classifier</th>
<th align="center">F1</th>
<th align="center">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td>XGBoost Optimized</td>
<td align="center">0.06</td>
<td align="center">0.71</td>
</tr>
</tbody>
</table>

<p>This model had the following confusion matrix:</p>

<center>
<img src="/images/capstone/cm_model8.png" width="400" />
</center>

<p>The point here is that this model presented a much more balanced confusion matrix which is, after all, our final goal. This behavior was better captured by the F1 score. This was the best F1 model but not the best AUC. In fact, AUC scores resembled the high scores obtained with accuracy. This is the reason why I consider that AUC is not the best metric for this problem.</p>

<h2 id="conclusions-and-next-steps"><strong>Conclusions and next steps</strong></h2>

<p>That was a quite interesting project, I enjoyed very much working on it. Here are the main conclusions I obtained from it:</p>

<ol>
  <li>the new integer NaN implemented in Pandas helped a lot with the cleaning</li>
  <li>outlier removal was very important for result improvement</li>
  <li>PCA and K-means were good tools for qualitative data analysis, specially if you want to use visualization to generate insights. However, for this dataset, they did not lead to better predictions</li>
  <li>Using XGBoost with sample weight parameters was a better solution than SMOTE</li>
  <li>Definitely you can get misled results if you use bad metrics</li>
</ol>

<p>We can still take a few steps to improve this model:</p>

<ol>
  <li>A more thorough T-SNE parameter search after performing PCA on the Mailout Train set. If T-SNE can spatially separate the classes, the problem would be solved without need for supervised learning</li>
  <li>Use DBSCAN instead of K-Means for cluster generation. This would probably not improve the customer segmentation analysis, but it could be helpful as an extra feature for supervised learning</li>
</ol>

  </div><a class="u-url" href="/jekyll/update/2019/07/21/capstone.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

  <!--  <h2 class="footer-heading">Bruno VC Martins</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">
            </li><li><a class="u-email" href="mailto:bvcm@posteo.org">bvcm@posteo.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/bvcmartins"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">bvcmartins</span></a></li><li><a href="https://www.linkedin.com/in/bvcmartins"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">bvcmartins</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>As you can see, this is my blog. So far  this is part of the requirements for my  Udacity&#39;s Data Scientist nanodegree. Eventually it will get a life of its own...</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
